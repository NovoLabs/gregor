# Gregor

[![PyPI](https://img.shields.io/pypi/l/Django.svg?style=plastic)]()
[![CircleCI](https://circleci.com/gh/NovoLabs/gregor/tree/master.svg?style=svg)](https://circleci.com/gh/NovoLabs/gregor/tree/master)


> As Gregor Samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect.

â€• Franz Kafka, _The Metamorphosis_

## Description

Gregor provides a channel-based API for asynchronously producing and consuming messages from [Apache Kafka](https://kafka.apache.org/).  Through the use of transducers, Gregor allows the user to transform data during production or consumption or both.

Gregor provides a control channel for interacting with the producer and consumer APIs.  Though it is not a complete implementation of the current Kafka producer and consumer objects, the current set of operations supports most use cases.  Additional control operations will be implemented as necessary.

## Inspirations

Gregor was inspired by [kinsky](https://github.com/pyr/kinsky) and [ring](https://github.com/ring-clojure).

## Dependencies

Since Gregor is an interface to Kafka, using it requires a Kafka instance.  To help you get up and running quickly, Gregor provides a `docker-compose.yaml` file that can be used to start up a local instance of Kafka.  Assuming you have [`docker-compose`](https://docs.docker.com/compose/install/) installed, you can copy [Gregor's docker compose file](https://github.com/NovoLabs/gregor/blob/master/docker/docker-compose.yaml) from GitHub to your local machine.

Once you copy Gregor's `docker-compose.yaml` file to your local machine (and `docker-compose` is installed), you can start Kafka with the following command:

```shell
$ docker-compose up
```

## Installation

To install Gregor, add the following to your Leiningen `:dependencies` vector:

```clojure
[codes.novolabs/gregor "0.1.0"]
```

## Usage

Gregor provides 2 namespaces public namespaces: `gregor.consumer` for creating and using a KafkaConsumer and `gregor.producer` for creating and using a KafkaProducer.

### Creating a Consumer

To create a consumer, we first need to pull the consumer namespace into our REPL:

```clojure
(require '[gregor.consumer :as c])
```

Assuming we have Kafka running on port 9092 on our local machine (the default location if we used the `docker-compose.yaml` file provided by Gregor), we can create a consumer connection using the following code:

```clojure
user> (def consumer (c/create {:output-policy #{:data :control :error}
                               :topics :gregor.test
                               :kafka-configuration {:bootstrap.servers "localhost:9092"
                                                     :group.id "gregor.consumer.test"}}))
;; => #'user/consumer
```

Lets take a closer look at the configuration map passed to `gregor.consumer/create`:

**`:output-policy`**

The value of `:output-policy` should be a set containing the types of events we want published to `out-ch`.  There are four types of events that Gregor supports:

* `:data` - Data events are generated by messages read from the configured topic or topics.  This event type is always included in a consumer's `:output-policy`, regardless of what is specified in the user-supplied `:output-policy`.  Without data events, the consumer would not be very useful.
* `:control` - Control events are generated by the output of control operations sent to the control channel.  We will talk more about the control channel and the supported operations below.
* `:error` - Error events are generated when errors occur, most commonly when an exception is thrown or invalid control operations are passed to the control channel.
* `:eof` - EOF events are sent when the consumer is closed by invoking the `:close` control operation.  Like `:data` events, the `:eof` event is always included as part of the `:output-policy` of a consumer.

**`:topics`**

The value of `:topics` can be a string, a keyword, a vector of strings and keywords or a regular expression:

* A string or keyword will subscribe the consumer to a single topic.  The string or keyword should be a [valid Kafka topic](https://stackoverflow.com/questions/37062904/what-are-apache-kafka-topic-name-limitations).
* A vector of strings and keywords will subscribe the consumer to each topic in the vector.  The vector can contain both strings and keywords.
* A regular expression will subscribe the consumer to all topics matching the regular expression.

You can check out the [KafkaConsumer documentation](https://kafka.apache.org/21/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html) for more information.

**`:kafka-configuration`**

The value of `:kafka-configuration` should be a map containing Kafka configuration options.  This map will be converted to a Java properties map and passed directly to the KafkaConsumer object during initialization.  The minimum requirements for consumer initialization are:

* `:bootstrap.servers` - A comma-delimited string containing `host:port` pairs inidicating the location of the Kafka server.
* `:group.id` - A string containing the group id of the consumer, which is used to maintain indexes and handle partitioning

You can check out the [Kafka consumer configuration documentation](https://kafka.apache.org/documentation/#consumerconfigs) for full treatment of all of the available configuration options.

### Consumer Control Operations

Assuming correct configuration, the result of calling `gregor.consumer/create` is a map containing 2 keys:

* `:out-ch` - The channel that receives all events, including `:data`, `:control`, `:error` and `:eof`.
* `:ctl-ch` - The channel that is used to send control operations

For the sake of conveneince, lets bind `out-ch` to the output channel and `ctl-ch` to the control channel:

```clojure
user> (def out-ch (:out-ch consumer))
;; => #'user/out-ch

user> (def ctl-ch (:ctl-ch consumer)) 
;; => #'user/ctl-ch
```

Now, lets check if we have been properly subscribed to the `gregor.test` topic.  We can query for the list of current subscriptions using the `:subscriptions` control operation.  The output will be written to `out-ch` as a `:control` event.  Here is the code:

```clojure
user> (require '[clojure.core.async :as a])

user> (a/>!! ctl-ch {:op :subscriptions})
;; => true

user> (-> (a/<!! out-ch) pr-str println)
;; => {:op :subscriptions, :subscriptions ["gregor.test"], :event :control}
```

This is a common pattern in Gregor:  The operation is submitted to the control channel and any results are pushed to the output channel as a `:control` event.

Lets look at each control operation that the consumer supports:

**`:subscribe`**

The `:subscribe` operation is used to subscribe to a topic(s).  Since Gregor requires that the initial topic(s) be passed in during initialization, the `:subscribe` operation will generally only be used if we wish to change the subscription of our consumer.  Should you need to do this, please remember that per the [KafkaConsumer documentation](https://kafka.apache.org/21/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html), **topic subscriptions are not incremental**.  If you wish to remain subscribed to the current list of topics, you will need to include these along with any additions when you call the `:subscribe` operation.

The code to call `:subscribe` looks like this:

```clojure
;; Subscribe to `:gregor.test.2`
user> (a/>!! ctl-ch {:op :subscribe :topics :gregor.test.2})
;; => true

;; Print the output from the `:subscribe` control operation
user> (-> (a/<!! out-ch) pr-str println)
;; => {:op :subscribe, :topics :gregor.test.2, :event :control}

;; Query for the list of current subscriptions using the `:subscriptions` operation
user> (a/>!! ctl-ch {:op :subscriptions})
;; => true

;; Print the output of the `:subscriptions` operation, noting that we are
;; now subscribed to the "gregor.test.2" topic
user> (-> (a/<!! out-ch) pr-str println)
;; => {:op :subscriptions, :subscriptions ["gregor.test.2"], :event :control}

;; Switch subscription back to "gregor.test"
user> (a/>!! ctl-ch {:op :subscribe :topics :gregor.test})
;; => true

;; Print the output of the `:subscribe` command
user> (-> (a/<!! out-ch) pr-str println)
;; => {:op :subscribe, :topics :gregor.test, :event :control}
```

**`:subscriptions`**

The `:subscriptions` operation, as we have seen, is used to get the list of topics that the consumer is currently subscribed to.  We have already seen how to call the `:subscriptions` operation above but we include the code here for the sake of completness:

```clojure
;; Query for the list of current subscriptions using the `:subscriptions` operation
user> (a/>!! ctl-ch {:op :subscriptions})
;; => true

;; Print the output of the `:subscriptions` operation
user> (-> (a/<!! out-ch) pr-str println)
;; => {:op :subscriptions, :subscriptions ["gregor.test"], :event :control}
```

If the consumer is not subscribed to a topic(s), the `:subscriptions` operation will return an empty vector

**`:unsubscribe`**

The `:unsubscribe` operation will unsubscribe the consumer from all current topic subscriptions.  It can be invoked using the following code:

```clojure
;; Unsubscribe from the current topic
user> (a/>!! ctl-ch {:op :unsubscribe})
;; => true

;; Print the output of the `:unsubscribe` operation
user> (-> (a/<!! out-ch) pr-str println)
;; => {:op :unsubscribe, :event :control}
```

If you are switching betwen a name-based subscription (i.e. strings or keywords) to a pattern-based subscription (i.e. a regular expression), you must unsubscribe.  If you want to change from one name-based subscription to another or one pattern-based subscription to another, you do not need to unsubscribe.

**`partitions-for`**

The `:partitions-for` operation will fetch the partition information for the specified topic.  You can use the following code to test `:partitions-for` for the `"gregor.test"` topic:

```clojure
;; Query the partitions for the `:gregor.test` topic
user> (a/>!! ctl-ch {:op :partitions-for :topic :gregor.test})
;; => true

;; Print the output of the `:partitions-for` operation for the `:gregor.test` topic
user> (-> (a/<!! out-ch) pr-str println)
;; => {:op :partitions-for, 
;;     :topic :gregor.test, 
;;     :partitions [{:type-name :partition-info, 
;;                   :isr [{:type-name :node, :host "127.0.0.1", :id 1, :port 9092}], 
;;                   :offline [], 
;;                   :leader {:type-name :node, :host "127.0.0.1", :id 1, :port 9092}, 
;;                   :partition 0, 
;;                   :replicas [{:type-name :node, :host "127.0.0.1", :id 1, :port 9092}], 
;;                   :topic "gregor.test"}], 
;;     :event :control}
```

**`commit`**

The `:commit` operation will commit the offsets from the last call to poll for the subscribed list of topics.  The default value of the `enable.auto.commit` property in Kafka is `true`, which means that as messages are consumed, the offset will be committed automatically (the interval of auto commits is controlled by the `auto.commit.interval.ms` property which has a default value of `500`).  If you set `enable.auto.commit` to `false` when you create your consumer you will need to manually commit consumed offsets, which can be done with the following code:

```clojure
;; Commit the last consumed offset for this consumer
user> (a/>!! ctl-ch {:op :commit})
;; => true

;; Verify that the `:commit` operation was processed succesfully
user> (-> (a/<!! out-ch) pr-str println)
;; => {:op :commit, :event :control}
```

Leaving `enable.auto.commit` set to the default value of `true` is sufficient for most use cases.  Read [this Medium article](https://medium.com/@danieljameskay/understanding-the-enable-auto-commit-kafka-consumer-property-12fa0ade7b65) for more information about auto committing and offsets.

**`:close`**

The `:close` operation will close the consumer, including all associated channels.  The following code will close the consumer we have been working with:

```clojure
;; Close the consumer
user> (a/>!! ctl-ch {:op :close})
;; true

;; Verify the `:close` operation was processed
user> (-> (a/<!! out-ch) pr-str println)
;; => {:op :close, :event :control}

;; Should receive `:eof` event as the final event indicating the end of the stream/channel
user> (-> (a/<!! out-ch) pr-str println)
;; => {:event :eof}

;; Further reads result in `nil` as the `out-ch` is closed
user> (-> (a/<!! out-ch) pr-str println)
;; => nil

;; Attempts to write to the `ctl-ch` will fail, returning `false`
(a/>!! ctl-ch {:op :subscriptions})
;; => false
```

Not only was the `KafkaConsumer` object closed, but so were `out-ch` and `ctl-ch` channels.  Since `out-ch` is a standard `core.async` channel, all messages that were read from the consumer up to the point of the `:close` operation will be avialable.  Once `out-ch` is empty, it wil return `nil` for all future reads.  Any attempts to write to `ctl-ch` will fail, returning `false` as shown above.

Note that the final message deliverd from `out-ch` was the `:eof` event.  Posting an `:eof` event is Gregor's way of telling the user that the stream has been closed.

### Creating a Producer

The other half of the equation is the producer.  Gregor provides a namespace called `gregor.producer` for creating and interacting with a KafkaProducer object.  The following code can be used to create a producer:

```clojure
(def producer (p/create {:output-policy #{:data :control :error}
                         :kafka-configuration {:bootstrap.servers "localhost:9092"}}))
```

As with the consumer, the configuration map we passed to `gregor.producer/create` is worth a closer look:

**`:output-policy`**

As with the consumer, the `:output-policy` is a set containing the types of events we want published to `out-ch`.  There are four types, the same as with the consumer.  However, the requirments and meaning is different in the context of a producer:

* `:data`: - Data events are generated by serializing the result of the call to the producer's `.send` function.  Note that serializing the result of `.send` is synchronous and, as such, including `:data` as part of a producer's output policy may affect throughput.
* `:control`: - Similar to the consumer, control events are generated by the output of control operations sent to the control channel.
* `:error`: - Similar to the consumer, error events are generated when errors occur, most commonly when an exception is thrown or invalid control operations are passed to the control channel.
* `:eof`: - EOF events are sent when the producer is closed by invoking the `:close` control operation.  The `:eof` event is always included as part of the `:output-policy` of a producer.

**`:kafka-configuration`**

The value of `:kafka-configuration` should be a map containing Kafka configuration options.  This map will be converted to a Java properties map and passed directly to the KafkaConsumer object during initialization.  The minimum requirements for consumer initialization are:

* `:bootstrap.servers` - A comma-delimited string containing `host:port` pairs inidicating the location of the Kafka server.

You can check out the [Kafka producer configuration documentation](https://kafka.apache.org/documentation/#producerconfigs) for a full treatment of all of the available configuration options.

### Producer Control Operations

Assuming correct configuration, the result of calling `gregor.producer/create` is a map containing 3 keys:

* `:in-ch` - The channel used to send `:data` events to the Kafka producer
* `:out-ch` - The channel that receives all events, including `:data`, `:control`, `:error` and `:eof`
* `:ctl-ch` - The channel used to send control operations

For the sake of conveneince, `out-ch` to the output channel and `ctl-ch` to the control channel:

```clojure
user> (def out-ch (:out-ch producer))
;; => #'user/out-ch

user> (def ctl-ch (:ctl-ch producer))
;; => #'user/ctl-ch
```

The producer currently supports three control operations:

**`:partitions-for`** 

The `:partitions-for` operation will fetch the partition information for the specified topic.  You can use the following code to test `:partitions-for` for the `"gregor.test"` topic:

```clojure
;; Query the partitions for the `:gregor.test` topic
user> (a/>!! ctl-ch {:op :partitions-for :topic :gregor.test})
;; => true

;; Print the output of the `:partitions-for` operation for the `:gregor.test` topic
user> (-> (a/<!! out-ch) pr-str println)
;; => {:op :partitions-for, 
;;     :topic :gregor.test, 
;;     :partitions [{:type-name :partition-info, 
;;                   :isr [{:type-name :node, :host "127.0.0.1", :id 1, :port 9092}], 
;;                   :offline [], 
;;                   :leader {:type-name :node, :host "127.0.0.1", :id 1, :port 9092}, 
;;                   :partition 0, 
;;                   :replicas [{:type-name :node, :host "127.0.0.1", :id 1, :port 9092}], 
;;                   :topic "gregor.test"}], 
;;     :event :control}
```

**`:flush`**

Invoking the `:flush` operation will make all buffered records immediately available to send.  Note that `:flush` will cause the producer thread to block until all buffered records have been sent.  You can invoke `:flush` with the following code:

```clojure
;; Invoke flush via the control channel
user> (a/>!! ctl-ch {:op :flush})
;; => true

;; Print the output of the `:flush` operation
user> (-> (a/<!! out-ch) pr-str println)
;; => {:op :flush, :event-ctl}
```

**`:close`**

The `:close` operation will close the producer, including all associated channels.  The following code will close the producer we have been working with:

```clojure
;; Close the consumer
user> (a/>!! ctl-ch {:op :close})
;; true

;; Verify the `:close` operation was processed
user> (-> (a/<!! out-ch) pr-str println)
;; => {:op :close, :event :control}

;; Should receive `:eof` event as the final event indicating the end of the stream/channel
user> (-> (a/<!! out-ch) pr-str println)
;; => {:event :eof}

;; Further reads result in `nil` as the `out-ch` is closed
user> (-> (a/<!! out-ch) pr-str println)
;; => nil

;; Attempts to write to the `ctl-ch` will fail, returning `false`
user> (a/>!! ctl-ch {:op :subscriptions})
;; => false

;; Attempts to write to the `in-ch` will fail, returning `false`
user> (a/>!! (:in-ch producer) {:foo :bar})
;; => false
```

Not only was the `KafkaProducer` object closed, but so were `in-ch`, `out-ch` and `ctl-ch` channels.  Since `out-ch` is a standard `core.async` channel, all messages that were read from the producer up to the point of the `:close` operation will be avialable.  Once `out-ch` is empty, it wil return `nil` for all future reads.  Any attempts to write to `ctl-ch` will fail, returning `false` as shown above.

Note that the final message deliverd from `out-ch` was the `:eof` event.  Posting an `:eof` event is Gregor's way of telling the user that the stream has been closed.

## License

Copyright Â© 2019 NovoLabs, Inc.

Distributed under the BSD-3-clause LICENSE
